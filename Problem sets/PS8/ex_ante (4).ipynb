{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum Likelihood Estimation of the Linear Model\n",
    "\n",
    "\n",
    "In this exercise we will consider the linear regression model. Of\n",
    "course, when estimated with OLS, the estimator, which minimizes the sum\n",
    "of squared residuals, has a closed form solution. This also goes for the\n",
    "maximum likelihood estimator when the residuals are assumed Gaussian. We\n",
    "will however do the maximization numerically using the scipy library and the `optimize` class, more specifically we will use its `minimize` function. The purpose of this exercise is to learn to do\n",
    "numerical maximization and to be familiar with $M$-estimators by viewing\n",
    "the maximum likelihood estimator of the linear model as an\n",
    "$M$-estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# general modules \n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    " \n",
    "# Code for this week \n",
    "import estimation_ante as est\n",
    "import LinearModel_ante as lm  \n",
    "\n",
    "# Set random seed\n",
    "seed = 42\n",
    "rng = np.random.default_rng(seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "plt.rcParams.update({\n",
    "    #\"text.usetex\": True, # LaTeX can sometimes be tricky to get working but makes graphs prettier :) \n",
    "    \"font.family\": \"serif\", \n",
    "    \"font.size\":18 \n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Model\n",
    "\n",
    "We consider a linear model with the following characteristics\n",
    "\n",
    "$$y_{i}=\\beta _{0}+\\beta _{1}x_{1i}+....+\\beta _{k-1}x_{k-1i}+u _{i} \\quad i=1,..,N$$\n",
    "\n",
    "with $u_{i} | X_i \\sim N(0,\\sigma^2)$.\n",
    "\n",
    "The (conditional) likelihood contribution for observation $i$ is,\n",
    "$$f\\left(y_{i}\\left|\\mathbf{x}_{i};\\beta,\\sigma^{2}\\right.\\right)\n",
    "  =\\frac{1}{\\sqrt{2\\pi\\sigma^{2}}} \\exp\\left\\{ -\\frac{1}{2} \\frac{\\hat{u}_{i}^{2}}{\\sigma^{2}}\\right\\},$$\n",
    "where\n",
    "$\\hat{u}_{i}=y_{i}- \\sum_{k=1}^K \\beta_k x_{ik}$. (Remember $\\hat{u}_{i}$ depends on $\\beta$)\n",
    "\n",
    "Thus, the loglikelihood contribution is \n",
    "$$\n",
    "\\ell_i(\\theta) = - \\frac{1}{2}\\log (2 \\pi) - \\frac{1}{2}\\log(\\sigma^2) - \\frac{1}{2}\\frac{\\hat{u}_i^2}{\\sigma^2}\n",
    "$$\n",
    "\n",
    "Often the term $-\\frac{1}{2}\\log\\left(2\\pi\\right)$ is dropped as it does not change with \n",
    "$\\beta$ or $\\sigma$ and thus does not affect the optimization. \n",
    "\n",
    "Finally, our optimizer will be solving the problem \n",
    "$$ \\min_\\theta \\frac{1}{N} \\sum_{i=1}^N q(\\theta, y_i, x_i),$$ \n",
    "where the criterion function is the negative loglikelihood, $q(\\theta,y_i,x_i) = -\\ell_i(\\theta)$. \n",
    "Again, we can drop the factor $N^{-1}$ as it does not affect the optimization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate dataset\n",
    "n = 100\n",
    "K = 2 # two regressors, a constant and one (real) regressor\n",
    "beta = np.ones((K,1))  # First is constant\n",
    "sigma = 3\n",
    "true_theta = np.vstack([beta, sigma])\n",
    "y, x = lm.sim_data(n, true_theta, rng)\n",
    "\n",
    "# Find some starting values \n",
    "theta0 = lm.starting_values(y, x, )\n",
    "theta0 = 0.8*theta0 # scale them by 0.8 to make the problem a little harder "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1: Write a function for the likelihood contribution.\n",
    "Open the file `LinearModel.py`, and fill in the function `loglikelihood` with the **likelihood contribution**. It should return an $N \\times 1$ vector of likelihood contributions.\n",
    "\n",
    "*Hint:* The sum of the likelihood contributions should be close to -162.8, given `theta0, x, y`, which I have written a function to check for you, so you know if you have written it correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in the missing parts of the lm.loglikelihood() function.\n",
    "# First, calculate the residual.\n",
    "# Then calculate the likelihood value, using the likelihood contribution equation from above.\n",
    "# Test if you got it right with the cell below. You might have to \"reseed\" by running the first cell in this notebook again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isclose(np.sum(lm.loglikelihood(theta0, y, x)), -162.800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: Estimate Parameters \n",
    "\n",
    "Now finish up the `estimation` function, which takes a function to minimize `func`, starting values `theta0`, the data `y` and `x`, and what type of variance to use `cov_type`.\n",
    "\n",
    "You need to use the `minimize` function, which takes the following inputs: the objective function `obj_func`, and the starting values `theta0`.\n",
    "\n",
    "You also need to finish up the `variance` function, which takes the function `func` (not the objective function), the data `y` and `x`, the results from the minimizer `result`, and finally what type of variance to calculate `cov_type`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.a Practice Estimation with `minimize`\n",
    "\n",
    "1. Create a `lambda` function, `Q`, taking only one input, `theta`, and returning the negative mean loglikelihood. \n",
    "    * ***Hint:*** Watch this video to learn about functions in Python: https://youtu.be/watch?v=loF8zsPaIjs. \n",
    "2. Evaluate `Q(theta0)` to test that it works. \n",
    "3. Call `minimize`, starting from `theta0`, with options having `disp` set to `True`, and the optimization algorithm (`method`) set to `BFGS`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. function handle to the objective \n",
    "Q = lambda theta : np.mean(lm.q(theta, y, x)) # just a function of one variable, theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.78886166]\n",
      " [1.08601912]\n",
      " [2.35222344]]\n"
     ]
    }
   ],
   "source": [
    "# 2. starting values \n",
    "print(theta0) # we defined these above, no need to redefine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 1.568403\n",
      "         Iterations: 15\n",
      "         Function evaluations: 64\n",
      "         Gradient evaluations: 16\n"
     ]
    }
   ],
   "source": [
    "# 3. call scipy minimize \n",
    "result = minimize(Q, theta0.flatten(), method='BFGS', options={'disp': True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get \n",
    "\n",
    "Optimization terminated successfully.\n",
    "\n",
    "         Current function value: 1.568403\n",
    "\n",
    "         Iterations: 15\n",
    "\n",
    "         Function evaluations: 64\n",
    "         \n",
    "         Gradient evaluations: 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3: Standard Errors\n",
    "\n",
    "**Tasks**: \n",
    "* Fill in `estimation.estimate()`: For the estimate function, you need to read the documentation for the optimize.minimize function, to pass the obj_func and theta0 to that function.\n",
    "    * ***Bonus:*** Make sure that `estimate()` passes the inputs `options` and the optional `kwargs` correctly to `optimize.minimize`. The options struct can e.g. ask the optimizer to print or not print final convergence output by setting `options = {'disp': True}` (or `False`). The `kwargs` can include things like controlling which algorithm is used for optimization, e.g. `method='BFGS'`. \n",
    "* Fill in `estimation.variance()`: implement all three options for $\\text{Avar}(\\hat{\\theta}$) described below. \n",
    "\n",
    "## Theory: The Three Asymptotic Variance Estimators\n",
    "\n",
    "The log-likelihood function is a nonlinear function, which must in\n",
    "general be maximized numerically in order to obtain the ML estimates.\n",
    "In general, for $M$-estimators, we know that \n",
    "$$\n",
    "\\sqrt{N}\\left( \\boldsymbol{\\hat{\\theta}}-\\boldsymbol{\\theta }_{0}\\right) \n",
    "\\overset{d}{\\rightarrow} \\mathcal{N} \\left(\\mathbf{0}, \\mathbf{A}_{0}^{-1} \\mathbf{B}_0 \\mathbf{A}_{0}^{-1} \\right). \n",
    "$$ \n",
    "\n",
    "For Maximum Likelihood (ML) estimators specifically, the *Information Matrix \n",
    "Equality* holds, which implies that \n",
    "$$ \\mathbf{A}_{0} = \\mathbf{B}_{0}. $$ \n",
    "This means that the asymptotic variance matrix simplifies so that \n",
    "$$ \\mathbf{A}_{0}^{-1} \\mathbf{B}_0 \\mathbf{A}_{0}^{-1} = \\mathbf{A}_{0}^{-1} = \\mathbf{B}_{0}^{-1}.$$ \n",
    "\n",
    "This means that we have three valid ways of estimating the asymptotic variance \n",
    "matrix of our parameter esimates: \n",
    "\n",
    "$\\widehat{\\text{Avar}}( \\boldsymbol{\\hat{\\theta}})$ can be taken to be any\n",
    "of the three options\n",
    "1. $\\widehat{\\text{Avar}}( \\boldsymbol{\\hat{\\theta}}) = N^{-1} \\hat{\\mathbf{A}}^{-1}$: the `Hessian`, \n",
    "2. $\\widehat{\\text{Avar}}( \\boldsymbol{\\hat{\\theta}}) = N^{-1} \\hat{\\mathbf{B}}^{-1}$: the `Outer Product`,  \n",
    "3. $\\widehat{\\text{Avar}}( \\boldsymbol{\\hat{\\theta}}) = N^{-1} \\hat{\\mathbf{A}}^{-1} \\hat{\\mathbf{B}} \\hat{\\mathbf{A}}^{-1} $: the `Sandwich`: viewed as a more \"robust\" option.  \n",
    "\n",
    "\n",
    "where the components matrices are\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\quad \\mathbf{\\hat{A}} \n",
    "    &= -\\frac{1}{N} \\left[ \\sum_{i=1}^{N}\\mathbf{H}_{i}( \\boldsymbol{\\hat{\\theta}}) \\right] \n",
    "\\\\\n",
    "\\quad  \\mathbf{\\hat{B}}\n",
    "    &= \\frac{1}{N}\n",
    "        \\left[ \\sum_{i=1}^{N} \\mathbf{s}_{i}( \\boldsymbol{\\hat{\\theta}}) \n",
    "                              \\mathbf{s}_{i}( \\boldsymbol{\\hat{\\theta}})^{\\prime}\n",
    "        \\right].\n",
    "\\end{aligned}\n",
    "$$ \n",
    "\n",
    "**Note**: Wooldridge has some confusing notation. In the notation above, the score, $\\mathbf{s}_{i}( \\boldsymbol{\\hat{\\theta}})$, and Hessian $\\mathbf{H}_{i}( \\boldsymbol{\\hat{\\theta}})$ are the first and second order derivatives of the log likelihood contribution, $\\ell_i(\\theta)$, evaluated at the estimated parameters, $\\boldsymbol{\\hat{\\theta}}$ - not the first and second order derivatives of $q(\\theta,y_i,x_i) = - \\ell_i(\\theta)$. This code implements an M-estimator that minimizes $q(\\theta,y_i,x_i)$. The only difference in the variance estimators is that there is no minus sign in front of the average Hessian in the expression of $\\mathbf{\\hat{A}}$. So when you implement the variance estimators you should not include the minus sign in front of the average Hessian.\n",
    "\n",
    "\n",
    "*Programming hint:* To calculate the variance you have to do the following:\n",
    "\n",
    "* `Hessian`: The ex ante code computes the *sum of numerical Hessians* $K \\times K$ matrix, `H_sum`. You need to compute the $K \\times K$ average Hessian, $\\mathbf{\\hat{A}}$. Finally, calculate the variance using:\n",
    "$$\n",
    "\\widehat{\\text{Avar}}( \\boldsymbol{\\hat{\\theta}}) = \\frac{1}{N}\\mathbf{\\hat{A}}^{-1}\n",
    "$$\n",
    "* `Outer Product`: the ex ante code computes the *numerical gradient* $N \\times K$ matrix, `s`. You need to compute the $K \\times K$ outer product of the scores, \n",
    "$ \\mathbf{s}' \\mathbf{s} = \\sum_{i=1}^N \\mathbf{s}_i \\mathbf{s}_i', $\n",
    "($\\mathbf{s}_i$ is $K \\times 1$ in this notation) and then use this to form $\\hat{\\mathbf{B}} = N^{-1} \\mathbf{s}' \\mathbf{s}$. Finally, calculate the variance using:\n",
    "$$\n",
    "\\widehat{\\text{Avar}}( \\boldsymbol{\\hat{\\theta}}) = \\frac{1}{N}\\mathbf{\\hat{B}}^{-1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.a: The 'Hessian' Estimator\n",
    "\n",
    "Compute standard errors using the `Hessian` estimator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "q() missing 2 required positional arguments: 'y' and 'x'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Bruger\\Advanced microeconometrics\\My-repository\\Problem sets\\PS8\\ex_ante (4).ipynb Cell 17\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Bruger/Advanced%20microeconometrics/My-repository/Problem%20sets/PS8/ex_ante%20%284%29.ipynb#X22sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m results_hes \u001b[39m=\u001b[39m est\u001b[39m.\u001b[39mestimate(lm\u001b[39m.\u001b[39mq, theta0, y, x, cov_type\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mHessian\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Bruger/Advanced%20microeconometrics/My-repository/Problem%20sets/PS8/ex_ante%20%284%29.ipynb#X22sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m label \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mbeta 1\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mbeta 2\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msigma2\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Bruger/Advanced%20microeconometrics/My-repository/Problem%20sets/PS8/ex_ante%20%284%29.ipynb#X22sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m est\u001b[39m.\u001b[39mprint_table(label, results_hes, title\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mMaximum Likelihood results\u001b[39m\u001b[39m'\u001b[39m, num_decimals\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Bruger\\Advanced microeconometrics\\My-repository\\Problem sets\\PS8\\estimation_ante.py:45\u001b[0m, in \u001b[0;36mestimate\u001b[1;34m(q, theta0, y, x, cov_type, options, **kwargs)\u001b[0m\n\u001b[0;32m     42\u001b[0m Q \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m theta: np\u001b[39m.\u001b[39mmean(q(theta, y, x))\n\u001b[0;32m     44\u001b[0m \u001b[39m# call optimizer\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m result \u001b[39m=\u001b[39m optimize\u001b[39m.\u001b[39mminimize(q,theta0) \u001b[39m# FILL IN: use \"optimize.minimize\" on the function Q, remember to give it **kwargs and options\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[39m# compute standard errors \u001b[39;00m\n\u001b[0;32m     49\u001b[0m cov, se \u001b[39m=\u001b[39m variance(q, y, x, result, cov_type)   \n",
      "File \u001b[1;32mc:\\Users\\Bruger\\anaconda3\\Lib\\site-packages\\scipy\\optimize\\_minimize.py:705\u001b[0m, in \u001b[0;36mminimize\u001b[1;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[0;32m    703\u001b[0m     res \u001b[39m=\u001b[39m _minimize_cg(fun, x0, args, jac, callback, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n\u001b[0;32m    704\u001b[0m \u001b[39melif\u001b[39;00m meth \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mbfgs\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m--> 705\u001b[0m     res \u001b[39m=\u001b[39m _minimize_bfgs(fun, x0, args, jac, callback, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n\u001b[0;32m    706\u001b[0m \u001b[39melif\u001b[39;00m meth \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mnewton-cg\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    707\u001b[0m     res \u001b[39m=\u001b[39m _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[0;32m    708\u001b[0m                              \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n",
      "File \u001b[1;32mc:\\Users\\Bruger\\anaconda3\\Lib\\site-packages\\scipy\\optimize\\_optimize.py:1418\u001b[0m, in \u001b[0;36m_minimize_bfgs\u001b[1;34m(fun, x0, args, jac, callback, gtol, norm, eps, maxiter, disp, return_all, finite_diff_rel_step, xrtol, **unknown_options)\u001b[0m\n\u001b[0;32m   1415\u001b[0m \u001b[39mif\u001b[39;00m maxiter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1416\u001b[0m     maxiter \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(x0) \u001b[39m*\u001b[39m \u001b[39m200\u001b[39m\n\u001b[1;32m-> 1418\u001b[0m sf \u001b[39m=\u001b[39m _prepare_scalar_function(fun, x0, jac, args\u001b[39m=\u001b[39margs, epsilon\u001b[39m=\u001b[39meps,\n\u001b[0;32m   1419\u001b[0m                               finite_diff_rel_step\u001b[39m=\u001b[39mfinite_diff_rel_step)\n\u001b[0;32m   1421\u001b[0m f \u001b[39m=\u001b[39m sf\u001b[39m.\u001b[39mfun\n\u001b[0;32m   1422\u001b[0m myfprime \u001b[39m=\u001b[39m sf\u001b[39m.\u001b[39mgrad\n",
      "File \u001b[1;32mc:\\Users\\Bruger\\anaconda3\\Lib\\site-packages\\scipy\\optimize\\_optimize.py:383\u001b[0m, in \u001b[0;36m_prepare_scalar_function\u001b[1;34m(fun, x0, jac, args, bounds, epsilon, finite_diff_rel_step, hess)\u001b[0m\n\u001b[0;32m    379\u001b[0m     bounds \u001b[39m=\u001b[39m (\u001b[39m-\u001b[39mnp\u001b[39m.\u001b[39minf, np\u001b[39m.\u001b[39minf)\n\u001b[0;32m    381\u001b[0m \u001b[39m# ScalarFunction caches. Reuse of fun(x) during grad\u001b[39;00m\n\u001b[0;32m    382\u001b[0m \u001b[39m# calculation reduces overall function evaluations.\u001b[39;00m\n\u001b[1;32m--> 383\u001b[0m sf \u001b[39m=\u001b[39m ScalarFunction(fun, x0, args, grad, hess,\n\u001b[0;32m    384\u001b[0m                     finite_diff_rel_step, bounds, epsilon\u001b[39m=\u001b[39mepsilon)\n\u001b[0;32m    386\u001b[0m \u001b[39mreturn\u001b[39;00m sf\n",
      "File \u001b[1;32mc:\\Users\\Bruger\\anaconda3\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:158\u001b[0m, in \u001b[0;36mScalarFunction.__init__\u001b[1;34m(self, fun, x0, args, grad, hess, finite_diff_rel_step, finite_diff_bounds, epsilon)\u001b[0m\n\u001b[0;32m    155\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf \u001b[39m=\u001b[39m fun_wrapped(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx)\n\u001b[0;32m    157\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_fun_impl \u001b[39m=\u001b[39m update_fun\n\u001b[1;32m--> 158\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_fun()\n\u001b[0;32m    160\u001b[0m \u001b[39m# Gradient evaluation\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcallable\u001b[39m(grad):\n",
      "File \u001b[1;32mc:\\Users\\Bruger\\anaconda3\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:251\u001b[0m, in \u001b[0;36mScalarFunction._update_fun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_update_fun\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    250\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf_updated:\n\u001b[1;32m--> 251\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_fun_impl()\n\u001b[0;32m    252\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf_updated \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Bruger\\anaconda3\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:155\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.update_fun\u001b[1;34m()\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mupdate_fun\u001b[39m():\n\u001b[1;32m--> 155\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf \u001b[39m=\u001b[39m fun_wrapped(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx)\n",
      "File \u001b[1;32mc:\\Users\\Bruger\\anaconda3\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:137\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnfev \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    134\u001b[0m \u001b[39m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[0;32m    135\u001b[0m \u001b[39m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[39m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m fx \u001b[39m=\u001b[39m fun(np\u001b[39m.\u001b[39mcopy(x), \u001b[39m*\u001b[39margs)\n\u001b[0;32m    138\u001b[0m \u001b[39m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39misscalar(fx):\n",
      "\u001b[1;31mTypeError\u001b[0m: q() missing 2 required positional arguments: 'y' and 'x'"
     ]
    }
   ],
   "source": [
    "results_hes = est.estimate(lm.q, theta0, y, x, cov_type='Hessian')\n",
    "\n",
    "label = ['beta 1', 'beta 2', 'sigma2']\n",
    "est.print_table(label, results_hes, title='Maximum Likelihood results', num_decimals=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results should look like this: \n",
    "\n",
    "|        | theta |    se |      t |\n",
    "|-------:|------:|------:|-------:|\n",
    "| beta 1 | 0.986 | 0.292 |  3.381 |\n",
    "| beta 2 | 1.358 | 0.377 |  3.604 |\n",
    "| sigma2 | 2.911 | 0.206 | 14.142 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.b: The 'Outer Product' Estimator\n",
    "\n",
    "Estimate parameters and compute standard errors using the `Outer Product` estimator. (This is the *default* variance estimator in `est.estimate`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_out = est.estimate(lm.q, theta0, y, x)\n",
    "est.print_table(label, results_out, title='Maximum Likelihood results', num_decimals=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your table should look a little like this: <br>\n",
    "\n",
    "Maximum Likelihood results <br>\n",
    "\n",
    "|        | theta |    se |      t |\n",
    "|-------:|------:|------:|-------:|\n",
    "| beta 1 | 0.986 | 0.306 |  3.225 |\n",
    "| beta 2 | 1.358 | 0.397 |  3.418 |\n",
    "| sigma2 | 2.911 | 0.230 | 12.656 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.c: The 'Sandwich' Estimator\n",
    "\n",
    "Compute standard errors using the `Sandwich` estimator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_san = est.estimate(lm.q, theta0, y, x, cov_type='Sandwich')\n",
    "est.print_table(label, results_san, title='Maximum Likelihood results', num_decimals=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results should look like this: \n",
    "\n",
    "|        | theta |    se |      t |\n",
    "|-------:|------:|------:|-------:|\n",
    "| beta 1 | 0.986 | 0.294 |  3.357 |\n",
    "| beta 2 | 1.358 | 0.362 |  3.753 |\n",
    "| sigma2 | 2.911 | 0.195 | 14.961 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4: Monte Carlo Study. \n",
    "\n",
    "**Task:** Conduct a Monte Carlo study for different sample sizes. \n",
    "\n",
    "Conduct a Monte Carlo study of the Maximum Likelihood estimator. Try\n",
    "various values of $N$ to illustrate the consistency of the\n",
    "estimator. Is the estimator biased? Compare the three types of\n",
    "standard error estimates to the Monte Carlo sampling standard\n",
    "deviation. Is the estimator of $\\sigma$ consistent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN = [5, 15, 50, 200]  # Sample size\n",
    "S = 1_000  # Number of replications\n",
    "P = len(theta0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize containers for all MX experiments\n",
    "theta_n = np.zeros((len(NN), P, S))\n",
    "se_theta_n = np.zeros((len(NN), P, S))\n",
    "MC_se = np.zeros((len(NN), P))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, N in enumerate(NN): # loop over sample sizes \n",
    "    print(f'N = {N:5d}: {i+1}/{len(NN)}')\n",
    "    for s in range(S): # for each Monte Carlo replication \n",
    "        y, x = lm.sim_data(N, true_theta, rng)\n",
    "        theta0 = lm.starting_values(y, x) * 0.8# find starting values, and scale them slightly by 0.8\n",
    "        results = est.estimate(lm.q, theta0, y, x, options={'disp':False}) # display off to avoid cluttering output \n",
    "        \n",
    "        theta_n[i, :, s] = results['theta_hat']\n",
    "        se_theta_n[i, :, s] = results['se']\n",
    "    \n",
    "    MC_se[i, :] = np.std(theta_n[i,:,:], axis=1, ddof=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "axes = axes.ravel() \n",
    "i_theta = 1 # second beta, i.e. not the constant \n",
    "theta_diff = theta_n[:, i_theta, :] - true_theta[i_theta, 0]\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.hist(theta_diff[i,:], bins=20)\n",
    "    ax.set_xlim(-10, 10)\n",
    "    ax.set_xlabel('$\\\\hat{\\\\theta}_1 - \\\\theta^o_1$')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "aa = np.linspace(0.3,0.5,len(NN))\n",
    "for i, N in enumerate(NN): \n",
    "    xx = np.linspace(-6,6,30)\n",
    "    ax.hist(theta_diff[i, :], bins=xx, alpha=aa[i], label=f'N = {N}', density=True)\n",
    "ax.legend(); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Same graph, scaled by $\\sqrt{N}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "axes = axes.ravel() \n",
    "i_theta = 1 # second beta, i.e. not the constant \n",
    "theta_diff = theta_n[:, i_theta, :] - true_theta[i_theta, 0]\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.hist(theta_diff[i,:] * np.sqrt(NN[i]), bins=30)\n",
    "    ax.set_xlim(-15, 15)\n",
    "    \n",
    "    ax.set_xlabel(f'$\\sqrt{{N}} (\\\\hat{{\\\\theta}}_{i_theta} - \\\\theta^o_{i_theta})$')\n",
    "    ax.set_ylabel('Monte carlo samples')\n",
    "    ax.set_title(f'N = {NN[i]}')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "aa = np.linspace(0.3,0.5,len(NN))\n",
    "for i, N in enumerate(NN): \n",
    "    xx = np.linspace(-15,15,30)\n",
    "    ax.hist(theta_diff[i, :] * np.sqrt(NN[i]), bins=xx, alpha=aa[i], label=f'N = {N}', density=True)\n",
    "ax.legend(); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4: Alternative Minimization Algorithms\n",
    "\n",
    "**Task:** Estimate the model using alternative minimization algorithms (the input `method` to `minimize`). Compare how many function evaluations they take and whether they converge to the global minimum. \n",
    "\n",
    "* `BFGS`: The default algorithm (Newton with approximated Hessian and numerical gradients), \n",
    "* `CG`: Newton with numerical Hessian and gradients,  \n",
    "* `Nelder-Mead`: Gradient-free optimizer, \n",
    "* `Powell`: Another gradient-free optimizer. \n",
    "\n",
    "***Hint:*** `est.estimate()` accepts various extra args, which are by default just passed to `scipy.optimize.minimize`. Try alternatives for `method` (the algorithm). \n",
    "\n",
    "***Note:*** The gradient-free optimizers do not return an inverse Hessian, so we can only compute the Outer Product variance matrix for those (unless we compute the Hessian numerically). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "y, x = lm.sim_data(N, true_theta, rng)\n",
    "theta0 = lm.starting_values(y, x)*0.8 # make the minimisation problem a little harder\n",
    "\n",
    "results_BFGS = est.estimate(lm.q, theta0, y, x, options={'disp':False}) # the default option is method='BFGS'\n",
    "results_CG   = # fill in # estimate using method='CG'\n",
    "results_NM   = # fill in # and with method='Nelder-Mead'\n",
    "results_PO   = # fill in # and with method='Powell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'--- BFGS ---')\n",
    "print(est.print_table(label, results_BFGS))\n",
    "\n",
    "print(f'\\n--- CG ---')\n",
    "print(est.print_table(label, results_CG))\n",
    "\n",
    "print(f'\\n--- Nelder-Mead---')\n",
    "print(est.print_table(label, results_NM))\n",
    "\n",
    "print(f'\\n--- Powell ---')\n",
    "print(est.print_table(label, results_PO))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected results (NB these are sensitive to how many times you've created synthetic data using the monte carlo simulations above - try re-running the entire notebook after restarting your kernel if your results dont match): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- BFGS ---\n",
    "# Optimizer succeeded after 14 iter. (60 func. evals.). Final criterion:    1.533.\n",
    "#Results\n",
    "#         theta      se        t\n",
    "# beta 1  0.6397  0.2862   2.2351\n",
    "# beta 2  0.4773  0.2920   1.6345\n",
    "# sigma2  2.8084  0.1992  14.0965\n",
    "\n",
    "# --- CG ---\n",
    "# Optimizer succeeded after 8 iter. (76 func. evals.). Final criterion:    1.533.\n",
    "# Results\n",
    "#          theta      se        t\n",
    "# beta 1  0.6397  0.2862   2.2354\n",
    "# beta 2  0.4774  0.2920   1.6345\n",
    "# sigma2  2.8084  0.1992  14.0964\n",
    "\n",
    "# --- Nelder-Mead ---\n",
    "# Optimizer succeeded after 64 iter. (116 func. evals.). Final criterion:    1.533.\n",
    "# Results\n",
    "#          theta      se        t\n",
    "# beta 1  0.6397  0.2862   2.2351\n",
    "# beta 2  0.4773  0.2921   1.6343\n",
    "# sigma2  2.8084  0.1992  14.0963\n",
    "\n",
    "# --- Powell ---\n",
    "# Optimizer succeeded after 2 iter. (69 func. evals.). Final criterion:    1.533.\n",
    "# Results\n",
    "#          theta      se        t\n",
    "# beta 1  0.6396  0.2862   2.2348\n",
    "# beta 2  0.4773  0.2921   1.6342\n",
    "# sigma2  2.8084  0.1992  14.0963"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "28b1ca885001685b2bc3a39df2ddd10ef1aa60bad7babe65fad257ff6b65d8b6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
